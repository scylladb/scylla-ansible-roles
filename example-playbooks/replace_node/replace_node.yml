---
# This play will make sure that the node being replaced is down and prevent it from trying to re-join
# the cluster by blocking its ip via iptables.
- name: Prepare nodes before replacement
  hosts: scylla
  vars_files:
    - vars/main.yml
  tasks:
    - name: Resolve a scylla_listen_address as a fact
      set_fact:
        listen_address: "{{ alive_nodes_listen_address }}"

    - name: Resolve scylla_broadcast_address as a fact
      set_fact:
        broadcast_address: "{{ alive_nodes_broadcast_address }}"

    - fail:
        msg: "The replaced node can't be a seed!"
      when: replaced_node_broadcast_address in scylla_seeds
      run_once: true

    - fail:
        msg: "The new node can't be a seed!"
      when: new_node in scylla_seeds
      run_once: true

    - name: Save one of the seeds as a fact in order to run some checks
      set_fact:
        alive_node: "{{ item }}"
      loop: "{{ scylla_seeds }}"
      run_once: true

    - name: Check if CQL port is up on {{ replaced_node_broadcast_address }}
      wait_for:
        port: "{{ cql_port }}"
        host: "{{ replaced_node_broadcast_address }}"
        timeout: 60
      register: _wait_for_cql_port_output
      ignore_errors: true
      delegate_to: "{{ alive_node }}"
      run_once: true

    - name: Validate that the node being replaced is down
      fail:
        msg: "The node {{ replaced_node }} must be down in order to be replaced!"
      when: _wait_for_cql_port_output.failed|bool == false
      run_once: true

    - name: Check if the other nodes are UP
      wait_for:
        port: "{{ cql_port }}"
        host: "{{ listen_address }}"
        timeout: 60
      when: inventory_hostname != new_node

    - name: Block replaced node's broadcast_address to prevent it from trying to join the cluster again
      iptables:
        chain: INPUT
        source: "{{ replaced_node_broadcast_address }}"
        jump: DROP
      become: true

    # By doing this we make sure that the new node will have the same io_properties as the others
    - name: Save io_properties from the seed node
      block:
        - name: store /etc/scylla.d/io_properties.yaml
          shell: |
            cat /etc/scylla.d/io_properties.yaml
          register: _io_properties

        - set_fact:
            io_properties: "{{ _io_properties.stdout }}"
      delegate_to: "{{ alive_node }}"
      run_once: true


# This play will install Scylla in the new node and update the existing nodes.
# The same variables used when the node role was executed for the replaced node should also
# be passed to this playbook and will be used by this play.
- name: Install/Update Scylla in all nodes
  hosts: scylla
  vars_files:
    - vars/main.yml
  vars:
    firewall_enabled: true
  roles:
    - ansible-scylla-node


# This play will set the ip/host_id of the replaced node in the scylla.yaml of the new node,
# start the new node, and then wait until the new node finishes joining the cluster. The same
# variables used when the node role was executed for the replaced node should also be passed
# to this playbook and will be used by this play.
- name: Start Scylla in the new node
  hosts: "{{ new_node }}"
  vars_files:
    - vars/main.yml
  become: true
  tasks:
    - name: Check if replace_node_first_boot is available in the current Scylla version
      shell: |
        scylla --help | grep replace-node-first-boot
      register: _replace_node_first_boot_grep
      ignore_errors: true

    - name: Set replace_node_first_boot
      block:
        - name: Get the host id for all nodes
          uri:
            url: "http://{{ scylla_api_address }}:{{ scylla_api_port }}/storage_service/host_id"
            method: GET
          register: _host_ids
          until: _host_ids.status == 200
          retries: 5
          delay: 1
          delegate_to: "{{ alive_node }}"

        - set_fact:
            _replaced_node_host_id: "{{ item.value }}"
          when: item.key == replaced_node
          loop: "{{ _host_ids.json }}"

        - name: Set replace_node_first_boot
          lineinfile:
            path: /etc/scylla/scylla.yaml
            regexp: '^(#\s*)?replace_node_first_boot:'
            line: "replace_node_first_boot: {{ _replaced_node_host_id }}"
            create: yes
      when: _replace_node_first_boot_grep.failed == false

    - name: Set replace_address_first_boot
      lineinfile:
        path: /etc/scylla/scylla.yaml
        regexp: '^(#\s*)?replace_address_first_boot:'
        line: "replace_address_first_boot: {{ replaced_node_broadcast_address }}"
        create: yes
      when: _replace_node_first_boot_grep.failed

    - name: Start Scylla
      service:
        name: scylla-server
        state: started

    - name: Wait for CQL port on {{ listen_address }}
      wait_for:
        port: "{{ cql_port }}"
        host: "{{ listen_address }}"
        timeout: "{{ new_node_bootstrap_wait_time_sec }}"

    - name: Wait for the added node to become healthy
      shell: |
        nodetool status|grep -E '^UN'|grep "{{ vars['ansible_'~scylla_nic].ipv4.address }}"| wc -l
      register: node_count
      until: node_count.stdout|int == 1
      retries: 300
      delay: 1
      delegate_to: "{{ item }}"
      loop: "{{ groups['scylla'] }}"

    - name: Remove the replace_node_first_boot record
      lineinfile:
        path: /etc/scylla/scylla.yaml
        regexp: '^replace_node_first_boot:'
        state: absent
      when: _replace_node_first_boot_grep.failed == false

    - name: Remove the replace_address_first_boot record
      lineinfile:
        path: /etc/scylla/scylla.yaml
        regexp: '^replace_address_first_boot:'
        state: absent
      when: _replace_node_first_boot_grep.failed

    - name: start and enable the Manager agent service
      service:
        name: scylla-manager-agent
        state: restarted
        enabled: yes

    - name: Check if RBNO is available in the current Scylla version
      shell: |
        scylla --help | grep enable-repair-based-node-ops
      register: _enable_rbno_grep
      ignore_errors: true

    - name: Check if RBNO was enabled for node replacement
      block:
        - command: cat /etc/scylla/scylla.yaml
          ignore_errors: true
          register: _scylla_yaml_out

        - set_fact:
            _scylla_yaml_map: "{{ _scylla_yaml_out.stdout | from_yaml }}"

        - set_fact:
            _rbno_enabled: "{{ _scylla_yaml_map.enable_repair_based_node_ops is not defined or _scylla_yaml_map.enable_repair_based_node_ops }}"
            _rbno_allowed_for_replace: "{{ _scylla_yaml_map.allowed_repair_based_node_ops is not defined or 'replace' in _scylla_yaml_map.allowed_repair_based_node_ops }}"
      when: _enable_rbno_grep.failed == false


- name: Update scylla-monitoring
  hosts: scylla-monitor
  roles:
    - ansible-scylla-monitoring


# This play will repair the new node if RBNO was not used and skip_repair is not set to true
- name: Repair the new node, if necessary
  hosts: scylla-manager
  vars_files:
    - vars/main.yml
  tasks:
    - name: Skip this play if RBNO was used for the replacement or if skip_repair is true
      meta: end_play
      when: skip_repair|bool or (hostvars[new_node]['_enable_rbno_grep'].failed == false and hostvars[new_node]['_rbno_enabled'] and hostvars[new_node]['_rbno_allowed_for_replace'])

    - name: Get cluster id
      shell: |
        sctool status | grep 'Cluster: {{ scylla_cluster_name }} ' | awk '{print $3}' | tr -d '()'
      register: _cluster_id

    - fail:
        msg: "Unable to find cluster {{ scylla_cluster_name }} in scylla-manager"
      when: _cluster_id.stdout|length == 0

    - name: Repair the new node
      shell: |
        sctool repair --cluster "{{ _cluster_id.stdout }}" --host "{{ new_node }}" {{ extra_repair_params }}
      register: _repair_id

    - name: Wait for the repair to finish
      shell:
        sctool progress "{{ _repair_id.stdout }}" --cluster "{{ _cluster_id.stdout }}" | grep "Status:" | awk '{print $2}'
      register: _repair_status
      until: _repair_status.stdout != "RUNNING"
      retries: "{{ new_node_repair_timeout_seconds|int // 30 }}" # retries = new_node_repair_timeout_seconds / delay
      delay: 30

    - fail:
        msg: "Repair failed!"
      when: _repair_status.stdout != "DONE"
